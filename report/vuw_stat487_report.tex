\documentclass{article}

\usepackage{graphicx, amsmath}

% 0. Referencing style, APA-like referencing
\usepackage[backend=biber, style=apa, natbib=true]{biblatex}
\addbibresource{bibliography.bib} % Replace 'yourbibfilename.bib' with your actual .bib file name.

% 1.  Don't need such wide margins.
\usepackage[margin=1in]{geometry} % Setting the margins to 1 inch

\usepackage{amsmath}

\begin{document}

\title{Ordinal Data clustering and prediction}

\author{Quan Zhao}

\maketitle

% \begin{abstract}
% \textbf{If you need an abstract, then it goes here. While you are reading this document, please also criticise it for the writing style, grammar, consistency, suitability, etc. }
% \end{abstract}

% 1. Search for relevant literature - 0:30
% 2. Evaluate and select sources -  0:58
% 3. Identify themes, debates, and gaps - 1:26
% 4. Outline your literature review's structure - 1:56
% 5. Write it - 2:34

\section{Introduction}
% introduction of the research topic
% (1/2 to 1 page).

% Finite mixture models are a flexible and powerful tool in statistical analysis, offering several key features that contribute to their widespread use in various fields.  These models assume that the observed data are generated from a combination of different probability distributions, each representing a distinct subpopulation within the overall population. The term "finite" in finite mixture models refers to the specific number of components or subpopulations in the mixture.

% Figure~\ref{fig:trend} shows the finite mixture modelling has strong trend in publications.

% intro of Ordinal Data

\subsection*{Ordinal Data}

Ordinal data, a pivotal concept in statistical analysis, represents categorical data characterized by a meaningful order among its categories, without implying uniform differences between these ranks. Unlike nominal data, which merely categorizes without any implied ranking, ordinal data elevates the categorical analysis by introducing a hierarchy or sequence that is significant yet lacks quantifiable intervals between its elements. This characteristic distinguishes ordinal data: it conveys the sequence of values but remains silent on the magnitude of difference between successive ranks.

Such data often appear as rankings or ordered classifications where the precise distance between categories is undefined or irrelevant. Despite sometimes being numerically coded for analytical convenience, ordinal data resist traditional arithmetic operations, rendering calculations like addition or subtraction inappropriate and misleading.

In the realm of statistical analysis, ordinal data necessitates specialized, non-parametric methods. The median and mode stand out as appropriate measures of central tendency for this data types, aligning with their ordered nature without assuming equal intervals between categories.

Ordinal data's prevalence is notably high in social sciences, especially in surveys and questionnaires designed to capture subjective assessments such as attitudes, opinions, and preferences. These instruments frequently employ scales—ranging from ``strongly disagree'' to ``strongly agree''—to elicit responses that, while ordered, do not support the notion of fixed distances between adjacent categories. This principle underlines the core attribute of ordinal data: the clear hierarchy among responses without an inherent interval scale to quantify the gaps between them. Consequently, assigning numerical values to such categories for computational purposes is generally discouraged and conceptually flawed, as it misinterprets the data's ordinal nature 
Thus, statements of the type
\[
\text{``disagree''} - \text{``strongly disagree''}
\]

\[
\text{``agree''} - \text{``don't know''}
\]
are not assumed. (Johnson, 2006~\textcite{Johnson1999})

\begin{figure}[ht!] % 'h!' places the figure here, in the text
    \centering % Centers the figure
    \includegraphics[width=0.6\textwidth]{images/ordinal_data_dist.png} % Include the image with 50% of the text width
    \caption{Latent trait interpretation of ordinal classification. 
    In this plot, the logistic density represents the distribution of latent traits for a particular individual. 
    It is assumed that a random variable is drawn from this density, and the value of this random variable determines an individual's classification. 
    For example, if a deviate of 0.5 is drawn, the individual receives a D. (Johnson, 2006~\cite{Johnson1999}).} % Caption for the image
    \label{fig:ordinal} % Label for referencing the figure in the text
  \end{figure}

\subsection*{Comparison with continuous data}

In the context of statistical analysis, the distinction between ordinal and continuous data types is pivotal, influencing both the choice of analytical methods and the depth of insights that can be derived. 
Ordinal data, characterized by its capacity to rank order categories without indicating precise differences between them, is inherently less informative than continuous data. 
Continuous data, with its quantitative nature, allows for an infinite range of values and supports detailed statistical operations, including arithmetic calculations and the application of advanced statistical models, facilitating a nuanced understanding of variables and their interrelations (Stevens, 1946~\cite{Stevens1946}).

The limitations of ordinal data stem from its inability to quantify the exact magnitude of differences between categories, a factor that restricts the application of parametric statistical methods. This constraint necessitates reliance on non-parametric methods, focusing on medians and modes rather than means and standard deviations, thereby offering a less detailed analysis (Conover, 1999~\cite{Conover1999}). 
For instance, in Likert scale responses commonly used in surveys, the ordinal nature of data precludes meaningful calculations of averages or differences between responses, limiting the depth of analysis that can be achieved (Likert, 1932~\cite{Likert1932}).

% task 7
Comparatively, continuous data's capacity for precise measurement and the application of a broader range of statistical analyses enables a more detailed and accurate exploration of phenomena. This difference underscores the importance of data type consideration in research design and analysis, as it directly impacts the validity and comprehensiveness of research findings.

In summary, while ordinal data is invaluable for capturing rankings and subjective assessments, 
its analytical limitations highlight the superior informational value of continuous data in quantitative research. 
This distinction is crucial for researchers in the selection of appropriate statistical methods and in the interpretation of their data, 
ensuring that conclusions drawn are both valid and meaningful.

\subsection*{Clustering}
% Task 8
% briefly introduce the topic of clustering in
% general. This section can be fairly short, e.g. just a few paragraphs to
% mention what clustering is and give examples of applications and briefly
% mention the two types of clustering (distance-based and model-based).


Due to the limitations of distance-based clustering approaches like K Neighbors, which struggle with the non-quantitative nature of ordinal data, statistical-based clustering methods offer a more effective solution. These methods adeptly handle the ordinal data's intrinsic ranking system, bypassing the need for precise numerical distances between categories.

% clustering

\section{Statistical-based Ordinal Data Clustering}

% Task 10, move before FMM.
\subsection{Stereotype Ordered Regression Model}

The Stereotype Ordered Regression Model is a statistical approach designed to analyze ordinal dependent variables, where the outcomes are categories with a natural order but not a quantifiable difference between them.

Given an ordinal response variable $Y$ with categories $1, 2, \ldots, J$, and a vector of independent variables $X$, the probability of $Y$ falling into the $j$th category, given $X = x$, is denoted as $P(Y = j | X = x)$.

The cumulative probability up to the $j$th category is defined as:
\[
P(Y \leq j | X = x) = \sum_{k=1}^{j} P(Y = k | X = x)
\]

The cumulative logit for category $j$ is then:
\[
\log\left(\frac{P(Y \leq j | X = x)}{1 - P(Y \leq j | X = x)}\right) = \theta_j + \beta^T x
\]
where $\theta_j$ is the intercept for category $j$, and $\beta$ is a vector of coefficients for $X$.

The Stereotype Model modifies the standard approach by positing:
\[
\beta_j = \gamma_j \beta
\]
for each category $j$ (except a reference category), where $\beta_j$ is the coefficient vector associated with category $j$, $\gamma_j$ is a scalar, 
and $\beta$ is a common vector of coefficients. This allows the effects to vary across categories but constrains them to follow a scaled version of a common pattern. (Anderson, 1984~\cite{anderson1984regression})

\subsection{Proportional Odds model}

The Proportional Odds model is another approach which defined for an ordinal response variable $Y$ with $J$ ordered categories ($1, 2, \ldots, J$). 
Given a set of predictor variables represented by the vector $X$, the model expresses the cumulative log odds of $Y$ being less than or equal to a category $j$ as follows:

\[
\log\left(\frac{P(Y \leq j | \mathbf{X})}{P(Y > j | \mathbf{X})}\right) = \theta_j - \mathbf{X}\boldsymbol{\beta}
\]

where:
\begin{itemize}
    \item $P(Y \leq j | \mathbf{X})$ is the cumulative probability of $Y$ being in category $j$ or any category below $j$, given the predictors $X$.
    \item $\theta_j$ is the intercept term for category $j$, which allows the log odds to vary across categories.
    \item $\mathbf{X}\boldsymbol{\beta}$ represents the linear predictor, with $\mathbf{X}$ being the matrix of predictor variables and $\boldsymbol{\beta}$ the vector of coefficients corresponding to these variables.
    \item The model assumes that the effect of the predictors on the log odds of being in a lower versus a higher category is proportional across all categories, which is encapsulated in the term $\mathbf{X}\boldsymbol{\beta}$.
\end{itemize}

The Proportional Odds assumption implies that the vector of coefficients $\boldsymbol{\beta}$ is the same across all category thresholds, facilitating a simpler interpretation and analysis. (McCullagh, 1980~\cite{mccullagh1980regression})


\subsection{Finite Mixture modeling}

Finite mixture models have emerged as a versatile and powerful statistical tool, garnering increasing attention across various scientific and research disciplines. These models, predicated on the assumption that observed data arise from a blend of several probability distributions, each representing a distinct subpopulation, have revolutionized our approach to understanding complex data structures. The designation ``finite'' in finite mixture models is pivotal, indicating the specific, but variable, number of components or subgroups within the data. This finite aspect offers a balance between model complexity and interpretability, allowing for detailed yet manageable analysis of diverse datasets.

The Finite mixture model is defined as:
\begin{equation}
p(x_i|\mathbf{\Theta}) = \sum_{k=1}^{K} \pi_k f(x_i|\theta_k)
\end{equation}
where:
\begin{itemize}
    \item $p(x_i|\mathbf{\Theta})$ denotes the overall mixture model's density or mass function for observation $x_i$.
    \item $K$ is the total number of component distributions in the mixture.
    \item $\pi_k$ represents the mixing proportion of the $k$th component, satisfying $0 \leq \pi_k \leq 1$ and $\sum_{k=1}^{K} \pi_k = 1$.
    \item $f(x_i|\theta_k)$ is the probability density function (pdf) or probability mass function (pmf) of the $k$th component distribution evaluated at $x_i$.
    \item $\theta_k$ denotes the parameter vector of the $k$th component distribution.
    \item $\mathbf{\Theta}$ symbolizes the complete set of parameters for the mixture model, including both the mixing proportions $\{\pi_1, \ldots, \pi_K\}$ and the parameters of the component distributions $\{\theta_1, \ldots, \theta_K\}$.
\end{itemize}

The significance of finite mixture models is vividly illustrated in Figure~\ref{fig:trend}, which showcases a robust upward trajectory in related publications. This trend not only reflects the growing academic and practical interest in these models but also underscores their evolving sophistication and broadening applicability. From the early advancements in maximizing likelihood estimation through algorithms like EM, as pioneered by McLachlan in 2000, to the more recent developments in handling a variety of data types, including binary, count, and ordinal data, finite mixture models have continuously adapted and expanded their scope.

\begin{figure}[ht!] % 'h!' places the figure here, in the text
    \centering % Centers the figure
    \includegraphics[width=0.6\textwidth]{images/trend.png} % Include the image with 50% of the text width
    \caption{The increase in publications indexed by PubMed that mention a keyword specific to cluster analyses relative to the number of publications 
    that mention a traditional statistical test. 
    Particularly sharp increases can be seen for finite mixture modelling.
    From~\cite{dalmaijer2022statistical}.} % Caption for the image
    \label{fig:trend} % Label for referencing the figure in the text
  \end{figure}


\section{History of Statistical based Ordinal Data Clustering}

\subsection*{Early Developments (2000--2010)}

McLachlan's 2000 paper marked a crucial step in mixture model applications by simplifying the maximum likelihood estimation (MLE) using the EM algorithm. This approach, utilizing $Y_j$ and $Z_j$, not only enhanced the computational efficiency of MLE but also laid a foundational strategy for Bayesian approaches and MCMC methods in mixture models. The paper’s impact is evident in its widespread adoption across various domains, from bioinformatics to finance, where mixture models are employed (McLachlan, 2000~\cite{mclachlan2000finite}).

In 2002, Figueiredo's introduction of an unsupervised algorithm for learning finite mixture models was a game-changer. This method's ability to autonomously select the number of components represented a significant leap over previous techniques, which often relied on arbitrary or manual component selection. Additionally, the algorithm's robustness against initialization issues and singular estimates made it a go-to choice for practitioners dealing with complex multivariate data (Figueiredo, 2002~\cite{figueiredo2002unsupervised}).

A key paper in 2010 by Volodymyr and Ranjan addressed practical challenges in applying the EM algorithm for mixture models. This comprehensive guide to estimation, model selection, and likelihood maximization was a boon for both researchers and practitioners. Notably, the work extended beyond Gaussian mixtures, offering insights and methodologies for simulating and visualizing non-Gaussian mixtures, thereby broadening the applicability of mixture models (Volodymyr and Ranjan, 2010~\cite{10.1214/09-SS053}).

\subsection*{Recent Developments (2010--2019)}

In 2016, Matechou's proposal of finite mixture models for biclustering two-mode ordinal categorical data introduced a novel approach to data analysis. By employing proportional odds parameterization, these models provided a nuanced understanding of complex data patterns, useful in fields such as genomics and social sciences where ordinal data is prevalent. The utilization of the EM algorithm for model-fitting underscored the enduring relevance of this method in mixture model applications (Matechou, 2016~\cite{matechou2016biclustering}).

Fernandez in 2016 offered an alternative methodology for clustering ordinal data. The use of likelihood-based methods through finite mixtures with the stereotype model presented a robust framework for analyzing complex data structures. This approach was particularly notable for its application in fuzzy clustering techniques, an area of growing interest in data science (Fernandez, 2016~\cite{fernandez2016mixture}).

Jacques' 2018 introduction of a model-based co-clustering algorithm was a significant advancement. The algorithm's ability to handle missing data and its interpretability made it especially relevant for high-dimensional datasets. The BOS distribution employed in this model underscored the continuous innovation in probabilistic modeling techniques, catering to the increasing complexity of data in modern research (Jacques, 2018~\cite{jacques2018model}).

Fernandez's 2019 extension of finite mixture models to binary, count, and ordinal data under a unified statistical framework represented a consolidation and expansion of mixture model applications. The introduction of maximum likelihood estimation parameters and the Bayesian approach for simultaneous estimation were indicative of the field's progression towards more flexible and comprehensive modeling techniques (Fernandez, 2019~\cite{fernandez2019finite}).

\subsection*{Compare with other clustering algorithm}

Compared to tree based or distance based clustering algorithms, statistical based clustering algorithms demonstrate superior performance with ordinal data, particularly in scenarios involving multiclass and multioutput cases. 
This advantage stems from two key factors: firstly, ordinal data do not presuppose equal distances between categories, a condition that distance-based methods often rely on. Secondly, ordinal data typically encompass only a few categories, which can limit the effectiveness of tree-based methods designed to partition data across a broader numerical range.
\section{Conclusion and Future Directions}

The evolution of mixture models from McLachlan's initial work in 2000 to advanced models in 2019 showcases a field marked by dynamic growth and innovation. These models have not only solved historical challenges but also set the stage for future advancements. As the field progresses, a key focus is integrating machine learning with traditional statistical methods, enhancing the capability of mixture models to accurately predict cluster membership for new data points. This is especially relevant in artificial intelligence applications, where mixture models contribute to improved pattern recognition and decision-making.

Furthermore, as data complexity and volume continue to escalate, the demand for robust, efficient, and versatile mixture models is more critical than ever. Future research is expected to concentrate on scaling up these models and improving their adaptability across diverse data types and structures. This progression underscores the mixture models' indispensable role in modern data analysis, poised to address the ever-growing challenges in data science and AI.

% \bibliographystyle{apacite}
% \bibliography{bibliography} % my .bib file

\printbibliography

\end{document}
